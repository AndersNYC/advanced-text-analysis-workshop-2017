{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Analysis with SpaCy and Scikit-Learn\n",
    "\n",
    "This notebook was originally prepared for the workshop [Advanced Text Analysis with SpaCy and Scikit-Learn](http://dhweek.nycdh.org/event/advanced-text-analysis-with-spacy-and-scikit-learn/), presented as part of NYCDH Week 2017. Here, we try out features of the SpaCy library for natural language processing. We also use some text analysis techniques from the Scikit-Learn library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Installing this software is easiest on a Linux-like system. If you're not already running Linux, you can easily download a distribution and copy it to a USB disk, which you can then boot from. I recommend getting [DH-USB](https://github.com/DH-Box/dh-usb), a Linux-based operating system made for the Digital Humanities. DH-USB already has all of this software installed. \n",
    "\n",
    "If you have a different Linux-like system, (including, to greater or lesser degrees, Ubuntu, MacOS, Cygwin, and Bash for Windows), you should be able to run these commands to install SpaCy, Scikit-Learn, Pandas, and the other required libraries. Ete3 is a library for tree visualization which is optional. \n",
    "\n",
    "```bash\n",
    "sudo pip install spacy scikit-learn pandas ete3\n",
    "```\n",
    "\n",
    "Note that if your system has Python 2 as the default, instead of Python 3, you might have to run `pip3` instead of `pip`. \n",
    "\n",
    "Now download the SpaCy data with this command: \n",
    "\n",
    "```bash\n",
    "python -m spacy.en.download all\n",
    "```\n",
    "\n",
    "To get my sent2tree library and all the sample data, simply `git clone` the repository where this notebook lives: \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/JonathanReeve/advanced-text-analysis-workshop-2017.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Display plots in this notebook, instead of externally. \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "%matplotlib inline\n",
    "\n",
    "# The following are optional dependencies. \n",
    "# Feel free to comment these out. \n",
    "# Sent2tree uses the sent2tree.py module in this repository. \n",
    "from sent2tree import sentenceTree\n",
    "import ete3 \n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This command might take a little while. \n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample data is the script of the 1975 film _Monty Python and the Holy Grail_, taken from the NLTK Book corpus, and the Project Gutenberg edition of Jane Austen's novel _Pride and Prejudice_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grail_raw = open('grail.txt').read()\n",
    "pride_raw = open('pride.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the texts. These commands might take a little while. \n",
    "grail = nlp(grail_raw)\n",
    "pride = nlp(pride_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Document\n",
    "\n",
    "Each SpaCy document is already tokenized into words, which are accessible by iterating over the document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pride[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pride[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also iterate over the sentences. `doc.sents` is a generator object, so we can use `next()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next(pride.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can force it into a list, and then do things with it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prideSents = list(pride.sents)\n",
    "prideSents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's find the longest sentence(s) in _Pride and Prejudice_: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prideSentenceLengths = [len(sent) for sent in prideSents]\n",
    "[sent for sent in prideSents if len(sent) == max(prideSentenceLengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Words\n",
    "\n",
    "Each word has a crazy number of properties: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pride[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[prop for prop in dir(pride[4]) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the indices (`.i`), we can make a lexical dispersion plot for the occurrences of that word in the novel. (This is just the SpaCy equivalent of the lexical dispersion plot from the NLTK Book, chapter 1.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pride[4].i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locations(needle, haystack): \n",
    "    \"\"\" \n",
    "    Make a list of locations, bin those into a histogram, \n",
    "    and finally put it into a Pandas Series object so that we\n",
    "    can later make it into a DataFrame. \n",
    "    \"\"\"\n",
    "    return pd.Series(np.histogram(\n",
    "        [word.i for word in haystack \n",
    "         if word.text.lower() == needle], bins=50)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I have no idea why I have to keep running this. \n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "\n",
    "pd.DataFrame(\n",
    "    {name: locations(name.lower(), pride) \n",
    "     for name in ['Elizabeth', 'Darcy', 'Jane', 'Bennet']}\n",
    ").plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can tell which characters end up getting together at the end, just based on this plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Named Entities\n",
    "\n",
    "Named entities can be accessed through `doc.ents`. Let's find all the types of named entities from _Monty Python and the Holy Grail_: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set([w.label_ for w in grail.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about those that are works of art? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[ent for ent in grail.ents if ent.label_ == 'WORK_OF_ART']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place names? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[ent for ent in grail.ents if ent.label_ == 'GPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(list([ent.string.strip() for ent in grail.ents if ent.label_ == 'ORG']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about groups of people? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set([ent.string for ent in grail.ents if ent.label_ == 'NORP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"French\" here refers to French _people_, not the French language. We can verify that by getting all the sentences in which this particular type of entity occurs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frenchPeople = [ent for ent in grail.ents if ent.label_ == 'NORP' and ent.string.strip() == 'French']\n",
    "[ent.sent for ent in frenchPeople]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech\n",
    "\n",
    "First, let's get the noun chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(pride.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a quick-and-dirty lookup table of POS IDs, \n",
    "# since the default representation of a POS is numeric. \n",
    "tagDict = {w.pos: w.pos_ for w in pride} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the distribution of parts of speech in these two texts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grailPOS = pd.Series(grail.count_by(spacy.attrs.POS))/len(grail)\n",
    "pridePOS = pd.Series(pride.count_by(spacy.attrs.POS))/len(pride)\n",
    "\n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "df = pd.DataFrame([grailPOS, pridePOS], index=['Grail', 'Pride'])\n",
    "df.columns = [tagDict[column] for column in df.columns]\n",
    "df.T.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see, for instance, what the most common punctuation marks might be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prideAdjs = [w for w in pride if w.pos_ == 'PUNCT']\n",
    "Counter([w.string.strip() for w in prideAdjs]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grailAdjs = [w for w in grail if w.pos_ == 'PUNCT']\n",
    "Counter([w.string.strip() for w in grailAdjs]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this on the level of a sentence. First, let's get all the sentences in which Sir Robin is explicitly mentioned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "robinSents = [sent for sent in grail.sents if 'Sir Robin' in sent.string]\n",
    "robinSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze just one of these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2 = robinSents[2]\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tags and parts of speech: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in r2: \n",
    "    print(word, word.tag_, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "Now let's analyse the structure of the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence has lots of properties: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[prop for prop in dir(r2) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drill down into the sentence, we can start with the root: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That root has children: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(r2.root.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all of the children for each word:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in r2: \n",
    "    print(word, ': ', str(list(word.children)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is very messy-looking, so let's create a nicer visualization. Here I'll be using a class I wrote called sentenceTree, available in the `sent2tree` module in this repository. It just shoehorns a SpaCy span (sentence or other grammatical fragment) into a tree that can be read by the `ete3` library for handling trees. This library just allows for some pretty visualizations of trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = sentenceTree(r2)\n",
    "t, ts = st.render()\n",
    "t.render('%%inline', tree_style=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already see how useful this might be. Since adjectives are typically children of the things they describe, we can get approximations for adjectives that describe characters. How is Sir Robin described? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in robinSents: \n",
    "    for word in sent: \n",
    "        if 'Robin' in word.string: \n",
    "            for child in word.children: \n",
    "                if child.pos_ == 'ADJ':\n",
    "                    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we shouldn't always trust syntactic insight! Now let's do something similar for Pride and Prejudice. First, we'll use named entity extraction to get a list of the most frequently mentioned characters:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter([w.string.strip() for w in pride.ents if w.label_ == 'PERSON']).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function that walks down the tree from each character, looking for the first adjectives it can find: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adjectivesDescribingCharacters(text, character):\n",
    "    sents = [sent for sent in pride.sents if character in sent.string]\n",
    "    adjectives = []\n",
    "    for sent in sents: \n",
    "        for word in sent: \n",
    "            if character in word.string:\n",
    "                for child in word.children: \n",
    "                    if child.pos_ == 'ADJ': \n",
    "                        adjectives.append(child.string.strip())\n",
    "    return Counter(adjectives).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try it on Mr. Darcy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adjectivesDescribingCharacters(pride, 'Darcy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same sort of thing, but look for associated verbs. First, let's get all the sentences in which Elizabeth is mentioned:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elizabethSentences = [sent for sent in pride.sents if 'Elizabeth' in sent.string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can peek at one of them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elizabethSentences[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = sentenceTree(elizabethSentences[3])\n",
    "t, ts = st.render()\n",
    "t.render('%%inline', tree_style=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the verb associated with Elizabeth, _remained_, not the root verb of the sentence, _walked_, which is associated with Mr. Darcy. So let's write a function that will walk up the dependency tree from a character's name until we get to the first verb. We'll use lemmas instead of the conjugated forms to collapse _remain_, _remains_, and _remained_ into one verb: _remain_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def verbsForCharacters(text, character):\n",
    "    sents = [sent for sent in pride.sents if character in sent.string]\n",
    "    charWords = []\n",
    "    for sent in sents: \n",
    "        for word in sent: \n",
    "            if character in word.string: \n",
    "                charWords.append(word)\n",
    "    charAdjectives = []\n",
    "    for word in charWords: \n",
    "        # Start walking up the list of ancestors \n",
    "        # Until we get to the first verb. \n",
    "        for ancestor in word.ancestors: \n",
    "            if ancestor.pos_.startswith('V'): \n",
    "                charAdjectives.append(ancestor.lemma_.strip())\n",
    "    return Counter(charAdjectives).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elizabethVerbs = verbsForCharacters(pride, 'Elizabeth')\n",
    "elizabethVerbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "darcyVerbs = verbsForCharacters(pride, 'Darcy')\n",
    "janeVerbs = verbsForCharacters(pride, 'Jane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now merge these counts into a single table, and then we can visualize it with Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def verbsToMatrix(verbCounts): \n",
    "    \"\"\" \n",
    "    Takes verb counts given by verbsForCharacters \n",
    "    and makes Pandas Series out of them, suitabe for combination in \n",
    "    a DataFrame. \n",
    "    \"\"\"\n",
    "    return pd.Series({t[0]: t[1] for t in verbCounts})\n",
    "\n",
    "verbsDF = pd.DataFrame({'Elizabeth': verbsToMatrix(elizabethVerbs), \n",
    "                        'Darcy': verbsToMatrix(darcyVerbs), \n",
    "                        'Jane': verbsToMatrix(janeVerbs)}).fillna(0)\n",
    "verbsDF.plot(kind='bar', figsize=(14,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilities\n",
    "\n",
    "SpaCy has a list of probabilities for English words, and these probabilities are automatically associated with each word once we parse the document. Let's see what the distribution is like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probabilities = [word.prob for word in grail] \n",
    "pd.Series(probabilities).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at some of the improbable words for _Monty Python and the Holy Grail_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set([word.string.strip().lower() for word in grail if word.prob < -19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some rudimentary information extraction by counting the improbable words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter([word.string.strip().lower() \n",
    "         for word in grail \n",
    "         if word.prob < -19.5]).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those words for _Pride and Prejudice_? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter([word.string.strip().lower() \n",
    "         for word in pride \n",
    "         if word.prob < -19.5 \n",
    "         and word.is_alpha\n",
    "         and word.pos_ != 'PROPN'] # This time, let's ignore proper nouns.\n",
    "       ).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this with ngrams, too, with some fancy Python magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ngrams(doc, n): \n",
    "    doc = [word for word in doc \n",
    "           if word.is_alpha # Get rid of punctuation\n",
    "           if not word.string.isupper()] # Get rid of all-caps speaker headings\n",
    "    return list(zip(*[doc[i:] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grailGrams = set(ngrams(grail, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram in grailGrams: \n",
    "    if sum([word.prob for word in gram]) < -40: \n",
    "        print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram in set(ngrams(pride, 3)): \n",
    "    if sum([word.prob for word in gram]) < -40: \n",
    "        print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings (Word Vectors)\n",
    "\n",
    "Word embeddings (word vectors) are numeric representations of words, usually generated via dimensionality reduction on a word cooccurrence matrix for a large corpus. The vectors SpaCy uses are the [GloVe](http://nlp.stanford.edu/projects/glove/) vectors, Stanford's Global Vectors for Word Representation. These vectors can be used to calculate semantic similarity between words and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coconut, africanSwallow, europeanSwallow, horse = nlp('coconut'), nlp('African Swallow'), nlp('European Swallow'), nlp('horse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coconut.similarity(horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "africanSwallow.similarity(horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "africanSwallow.similarity(europeanSwallow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at vectors for _Pride and Prejudice_. First, let's get the first 150 nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prideNouns = [word for word in pride if word.pos_.startswith('N')][:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get vectors and labels for each of them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prideNounVecs = [word.vector for word in prideNouns]\n",
    "prideNounLabels = [word.string.strip() for word in prideNouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prideNounVecs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A single vector is 300-dimensional, so in order to plot it in 2D, it might help to reduce the dimensionality to the most meaningful dimensions. We can use Scikit-Learn to perform truncated singular value decomposition for latent semantic analysis (LSA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=2)\n",
    "lsaOut = lsa.fit_transform(prideNounVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results in a scatter plot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs, ys = lsaOut[:,0], lsaOut[:,1]\n",
    "for i in range(len(xs)): \n",
    "    plt.scatter(xs[i], ys[i])\n",
    "    plt.annotate(prideNounLabels[i], (xs[i], ys[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vectorization\n",
    "\n",
    "This uses a non-semantic technique for vectorizing documents, just using bag-of-words. We won't need any of the fancy features of SpaCy for this, just scikit-learn. We'll use a subset of the Inaugural Address Corpus that contains 20th and 21st century inaugural addresses. \n",
    "\n",
    "First, we'll vectorize the corpus using scikit-learn's `TfidfVectorizer` class. This creates a matrix of word frequencies. (It doesn't actually use TF-IDF, since we're turning that off in the options below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(input='filename', decode_error='ignore', use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inauguralFilenames = sorted(glob('inaugural/*'))\n",
    "\n",
    "# Make labels by removing the directory name and .txt extension: \n",
    "labels = [filename.split('/')[1] for filename in inauguralFilenames]\n",
    "labels = [filename.split('.')[0] for filename in labels]\n",
    "\n",
    "# While we're at it, let's make a list of the lengths, so we can use them to plot dot sizes. \n",
    "lengths = [len(open(filename, errors='ignore').read())/100 for filename in inauguralFilenames]\n",
    "\n",
    "# Add a manually compiled list of presidential party affiliations, \n",
    "# So that we can use this to color our dots. \n",
    "parties = 'rrrbbrrrbbbbbrrbbrrbrrrbbrrbr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfOut = tfidf.fit_transform(inauguralFilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfOut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsaOut = lsa.fit_transform(tfidfOut.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs, ys = lsaOut[:,0], lsaOut[:,1]\n",
    "for i in range(len(xs)): \n",
    "    plt.scatter(xs[i], ys[i], c=parties[i], s=lengths[i], alpha=0.5)\n",
    "    plt.annotate(labels[i], (xs[i], ys[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Sentence Lengths\n",
    "\n",
    "Let's load the Inaugural Address documents into SpaCy to analyze things like average sentence length. SpaCy makes this really easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inaugural = [nlp(open(doc, errors='ignore').read()) for doc in inauguralFilenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentLengths = [ np.mean([len(sent) for sent in doc.sents]) for doc in inaugural ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(sentLengths, index=labels).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Distributions\n",
    "\n",
    "This sort of thing you've probably already seen in the NLTK book, but it's made even easier in SpaCy. We're simply going to count the occurrences of words and divide by the total number of words in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inauguralSeries = [pd.Series(Counter(   \n",
    "                    [word.string.strip().lower() \n",
    "                     for word in doc]))/len(doc) \n",
    "                     for doc in inaugural]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seriesDict = {label: series for label, series in zip(labels, inauguralSeries)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inauguralDf = pd.DataFrame(seriesDict).T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inauguralDf[['america', 'world']].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "americaWorldRatio = inauguralDf['america']/inauguralDf['world']\n",
    "americaWorldRatio.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarities = [ [doc.similarity(other) for other in inaugural] for doc in inaugural ]\n",
    "similaritiesDf = pd.DataFrame(similarities, columns=labels, index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requires the Seaborn library. \n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "seaborn.heatmap(similaritiesDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Extract all the events from _Pride and Prejudice_. \n",
    "2. Make a lexical dispersion plot of the word \"ni\" in _Monty Python and the Holy Grail_. What does this tell us? \n",
    "3. Find the shortest sentence in any inaugural address from our corpus.\n",
    "4. Find the president that used the lowest proportions of adjectives (or nouns, verbs) in his inaugural address. \n",
    "5. Find which of Charles Dickens's novels (or those of any other author) are the most semantically similar. \n",
    "\n",
    "# Learn More\n",
    "\n",
    " - [SpaCy Homepage](https://spacy.io/)\n",
    " - [Pycon: NLP in 10 Lines of Code](https://github.com/cytora/pycon-nlp-in-10-lines)\n",
    " - [What You Can Learn About Food By Analyzing a Million Yelp Reviews](http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb)\n",
    " - [Other Tutorials Listed on Spacy.io](https://spacy.io/docs/usage/tutorials)\n",
    " \n",
    "# See Also\n",
    "\n",
    " - [Textacy, higher-level NLP based on SpaCy](https://github.com/chartbeat-labs/textacy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
